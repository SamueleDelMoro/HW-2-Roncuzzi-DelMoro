{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b189f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all packages\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import os, sys\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "198a4dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A', 'F', 'A', 'Gm', 'A#', 'A#', 'Gm', 'F', 'F', 'F', 'A', 'A', 'Gm', 'F', 'F', 'F', 'F', 'F', 'Gm', 'F', 'F', 'A#', 'A#', 'Gm', 'Gm', 'F', 'F', 'F', 'F', 'Gm', 'F', 'F', 'Gm', 'A#', 'A#', 'A', 'F', 'F', 'F', 'F', 'F', 'Gm', 'F', 'F', 'A#', 'A#', 'Gm', 'F', 'F', 'A', 'F', 'F', 'A#', 'F', 'F', 'A#', 'A#', 'Gm', 'F', 'F', 'A#', 'F', 'F', 'F', 'A#', 'F', 'Gm', 'Gm', 'A#', 'Gm', 'A#', 'F', 'A', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A', 'Gm', 'F', 'A#', 'A', 'F', 'A#', 'A#', 'F', 'F', 'A#', 'A', 'A#', 'A#', 'F', 'F', 'A#', 'A#', 'F', 'F', 'Gm', 'A#', 'A', 'A', 'A', 'Gm', 'F', 'A', 'A', 'A', 'A#', 'A', 'A', 'A', 'F', 'A#', 'A#', 'F', 'F', 'F', 'A#', 'F', 'A#', 'F', 'F', 'A#', 'A#', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A', 'D', 'D', 'D', 'D', 'F', 'A#', 'A#', 'D', 'F', 'A#', 'A#', 'A#', 'A#', 'D', 'D', 'F', 'D', 'A#', 'D', 'A#', 'A', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'F', 'A#', 'A#', 'A', 'A', 'Gm', 'F', 'Gm', 'A#', 'F', 'F', 'A#', 'F', 'A', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'Gm', 'Gm', 'A#', 'A#', 'A#', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'D', 'A', 'A', 'Gm', 'D', 'D', 'Gm', 'F', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'A#', 'A#', 'D', 'A#', 'A#', 'A#', 'A', 'A#', 'F', 'D', 'D', 'D', 'A', 'Gm', 'Gm', 'F', 'F', 'D', 'D', 'F', 'Gm', 'Gm', 'A#', 'F', 'A#', 'F', 'A#', 'A#', 'F', 'A', 'A', 'A#', 'A#', 'A#', 'F', 'Gm', 'F', 'F', 'F', 'A#', 'A#', 'A', 'Gm', 'F', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'Gm', 'A', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'A', 'F', 'F', 'A', 'A', 'A', 'Gm', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'D#', 'A', 'A', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A', 'Gm', 'F', 'F', 'F', 'F', 'Gm', 'A#', 'A#', 'A#', 'A', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'D', 'F', 'A', 'A', 'A', 'A#', 'A#', 'A', 'A', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'F', 'A#', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'A#', 'A', 'F', 'A#', 'A#', 'A', 'F', 'F', 'F', 'F', 'F', 'A', 'A', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'A', 'F', 'A#', 'A', 'A', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A', 'A', 'D', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A', 'A', 'A#', 'D', 'D', 'A', 'A', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'A#', 'F', 'F', 'A', 'A#', 'A', 'F', 'F', 'F', 'A', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A', 'F', 'Gm', 'F', 'A#', 'Gm', 'A', 'A', 'A', 'F', 'A#', 'A', 'A', 'Gm', 'Gm', 'A', 'A', 'Gm', 'A#', 'A#', 'F', 'A#', 'Gm', 'Gm', 'A#', 'F', 'F', 'A', 'A#', 'A#', 'A#', 'F', 'D', 'D', 'A#', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'D', 'A#', 'F', 'Gm', 'A', 'F', 'D', 'A#', 'F', 'A#', 'A#', 'Gm', 'D', 'D', 'A', 'F', 'A#', 'D', 'D', 'D', 'A#', 'F', 'A#', 'D', 'A', 'A#', 'F', 'F', 'A#', 'F', 'F', 'F', 'F', 'A#', 'A#', 'Gm', 'A#', 'F', 'A#', 'F', 'F', 'A#', 'A#', 'F', 'F', 'A#', 'A', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'Gm', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'A#', 'Gm', 'F', 'D', 'D', 'D', 'F', 'A#', 'F', 'F', 'A#', 'D', 'A#', 'F', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'Gm', 'A#', 'A#', 'F', 'A#', 'A#', 'A', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'F', 'D#', 'F', 'Gm', 'F', 'F', 'F', 'A#', 'A', 'A', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'A', 'A#', 'A', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A', 'F', 'F', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A', 'Gm', 'F', 'F', 'F', 'F', 'Gm', 'F', 'F', 'A#', 'A', 'F', 'A', 'Gm', 'F', 'Gm', 'F', 'A#', 'F', 'D', 'A#', 'A', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A#', 'F', 'Gm', 'A#', 'A', 'A', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'A#', 'F', 'D', 'A', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A', 'F', 'F', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'Gm', 'A', 'Gm', 'F', 'F', 'Gm', 'F', 'F', 'F', 'A', 'Gm', 'F', 'A#', 'F', 'F', 'F', 'F', 'A#', 'A#', 'D', 'A#', 'A#', 'F', 'F', 'A#', 'A', 'A#', 'A#', 'A#', 'A#', 'A#', 'D', 'D', 'F', 'A#', 'A#', 'Gm', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A', 'F', 'Gm', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A', 'A', 'Gm', 'F', 'A', 'F', 'A#', 'A#', 'A#', 'Gm', 'D', 'A#', 'A#', 'D', 'Gm', 'A#', 'Gm', 'Gm', 'A#', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'F', 'F', 'A#', 'D', 'D', 'D', 'A#', 'F', 'F', 'F', 'F', 'A', 'Gm', 'A', 'A', 'F', 'A', 'A', 'A', 'A', 'A', 'A', 'Gm', 'A#', 'A', 'F', 'F', 'A#', 'F', 'A#', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'F', 'F', 'A', 'Gm', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'D#', 'F', 'Gm', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'F', 'F', 'F', 'F', 'F', 'F', 'D', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'Gm', 'F', 'F', 'D', 'D', 'F', 'F', 'D', 'D', 'F', 'F', 'A#', 'A#', 'D', 'D', 'D', 'D', 'F', 'A#', 'F', 'A#', 'F', 'A#', 'A', 'A', 'A', 'A', 'A#', 'A', 'A', 'A', 'A#', 'A#', 'A#', 'D', 'A#', 'D', 'F', 'A#', 'F', 'A', 'Gm', 'Gm', 'A', 'A', 'A', 'A', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'D#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'A#', 'A', 'F', 'A', 'F', 'A#', 'Gm', 'A', 'A#', 'A', 'A', 'A', 'Gm', 'F', 'A#', 'A', 'A', 'F', 'F', 'A#', 'F', 'F', 'Gm', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'D', 'D', 'D', 'A', 'F', 'F', 'D', 'A#', 'D', 'F', 'A#', 'F', 'A#', 'F', 'D', 'D', 'A#', 'A#', 'A#', 'D', 'D', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'A', 'F', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'A#', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'D', 'D', 'A#', 'A', 'Gm', 'D#', 'D', 'D', 'F', 'A#', 'F', 'Gm', 'A#', 'A#', 'Gm', 'A#', 'F', 'A#', 'D', 'F', 'A#', 'A#', 'A#', 'A#', 'Gm', 'F', 'F', 'F', 'Gm', 'F', 'F', 'F', 'F', 'Gm', 'A#', 'Gm', 'Gm', 'F', 'Gm', 'Gm', 'F', 'F', 'A#', 'A#', 'A#', 'D#', 'D', 'A', 'A#', 'A#', 'A#', 'Gm', 'A', 'F', 'F', 'F', 'A#', 'A#', 'A', 'A', 'A', 'A#', 'F', 'A#', 'F', 'A', 'F', 'F', 'A#', 'A#', 'F', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A', 'F', 'F', 'F', 'F', 'F', 'A', 'A', 'A#', 'A#', 'F', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'F', 'F', 'F', 'A#', 'A#', 'F', 'A#', 'Gm', 'F', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'A#', 'A#', 'A', 'A', 'A#', 'A#', 'A#', 'A', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'Gm', 'Gm', 'A', 'A', 'A', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'A', 'Gm', 'F', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'A', 'Gm', 'F', 'F', 'F', 'F', 'F', 'A', 'A', 'A#', 'F', 'F', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'F', 'A', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'D', 'A', 'A', 'A', 'A#', 'A#', 'F', 'Gm', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'Gm', 'F', 'F', 'F', 'F', 'F', 'F', 'A', 'F', 'F', 'F', 'A', 'Gm', 'Gm', 'Gm', 'F', 'A#', 'F', 'F', 'A#', 'A#', 'A#', 'A', 'F', 'A#', 'Gm', 'Gm', 'D#', 'F', 'F', 'D', 'D', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'A#', 'A#', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'D', 'D', 'D', 'A#', 'A#', 'D', 'D#', 'A', 'F', 'A#', 'D', 'D', 'F', 'F', 'D#', 'F', 'A#', 'Gm', 'D#', 'F', 'A', 'A', 'F', 'A', 'A', 'A', 'A', 'A', 'A', 'Gm', 'A', 'A', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'F', 'F', 'A', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'D#', 'F', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'D#', 'A#', 'F', 'F', 'A#', 'A#', 'A#', 'D', 'D', 'F', 'A#', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A', 'D', 'D', 'A#', 'F', 'A#', 'A#', 'D', 'A', 'D#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'Gm', 'A', 'A', 'A#', 'F', 'A', 'Gm', 'Gm', 'Gm', 'A', 'A', 'D', 'A#', 'D', 'D', 'A#', 'Gm', 'Gm', 'A#', 'Gm', 'A', 'A', 'A#', 'A#', 'A#', 'A', 'A#', 'A#', 'A#', 'A#', 'D', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'F', 'D#', 'A#', 'A#', 'A', 'A#', 'F', 'F', 'A#', 'A', 'A', 'A#', 'A', 'A', 'A', 'A', 'A', 'A', 'A#', 'A#', 'A', 'A', 'A', 'A#', 'A#', 'A#', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'D', 'A', 'F', 'F', 'Gm', 'A#', 'A#', 'A#', 'A#', 'A', 'F', 'A#', 'A#', 'A#', 'D', 'D', 'D', 'D', 'D', 'F', 'D', 'A#', 'A#', 'A#', 'D', 'F', 'A#', 'D', 'D', 'A#', 'A#', 'A#', 'A#', 'D', 'A', 'A', 'F', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'Gm', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'F', 'A', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'Gm', 'D', 'D', 'D#', 'F', 'A#', 'A#', 'D', 'D', 'D', 'A#', 'D', 'D', 'A#', 'F', 'F', 'A#', 'F', 'F', 'F', 'D', 'Gm', 'A#', 'F', 'A#', 'A#', 'D', 'A#', 'F', 'F', 'F', 'A', 'F', 'Gm', 'F', 'F', 'F', 'F', 'A#', 'A', 'Gm', 'F', 'F', 'A#', 'A#', 'D', 'D', 'D', 'F', 'F', 'F', 'F', 'A#', 'F', 'Gm', 'Gm', 'F', 'F', 'A#', 'A', 'A', 'A', 'A', 'F', 'A#', 'F', 'A#', 'A#', 'F', 'F', 'A', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'D', 'A#', 'F', 'F', 'A#', 'A#', 'Gm', 'A', 'F', 'F', 'F', 'F', 'F', 'A', 'A', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'F', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'Gm', 'A', 'A', 'F', 'A#', 'A#', 'A#', 'A#', 'F', 'A#', 'F', 'A', 'A', 'A#', 'D', 'D', 'A#', 'A', 'Gm', 'A#', 'A#', 'A#', 'D', 'F', 'F', 'F', 'Gm', 'F', 'F', 'F', 'F', 'A', 'F', 'A', 'A', 'A', 'A#', 'A', 'A#', 'F', 'F', 'A#', 'Gm', 'F', 'A#', 'Gm', 'A#', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'D', 'A#', 'F', 'F', 'A#', 'A#', 'A', 'A#', 'A#', 'F', 'F', 'F', 'F', 'A#', 'A#', 'F', 'F', 'A#', 'F', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'A#', 'A#', 'A#', 'A#', 'Gm', 'Gm', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'D', 'A', 'A#', 'A', 'A', 'F', 'F', 'F', 'A', 'A', 'A#', 'A#', 'A#', 'A#', 'F', 'D', 'F', 'F', 'F', 'F', 'A#', 'F', 'F', 'F', 'A#', 'F', 'A', 'Gm', 'F', 'A#', 'Gm', 'A#', 'A#', 'D#', 'F', 'A', 'A#', 'F', 'A#', 'A#', 'A#', 'A#', 'F', 'F', 'F', 'Gm', 'F', 'F', 'F', 'A', 'A#', 'A#', 'F', 'F', 'A#', 'D', 'A#', 'A#', 'A#', 'D', 'Gm', 'A', 'A#', 'A#', 'F', 'A#', 'A#', 'A#', 'D', 'A#', 'D', 'A#', 'Gm', 'D#', 'F', 'F', 'D#', 'A#', 'F', 'F', 'A', 'A#', 'F', 'F', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'F', 'A#', 'A#', 'F', 'F', 'D', 'A#', 'A', 'A#', 'F', 'A', 'F', 'A#', 'F', 'F', 'F', 'F', 'F', 'F', 'A', 'F', 'D', 'D', 'F', 'A', 'F', 'A#', 'A', 'A#', 'D', 'D', 'D']\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "'''Implement the template based chord recognition algorithm. Define a function that takes as input the\n",
    "path to a wav file and returns the estimated chords sequence labels.\n",
    "The output must be a list\n",
    "\n",
    "λ_pred = [λ_pred0, λpred_1, ..., λpred_N−1] (1)\n",
    "\n",
    "where each element λpred_n is the predicted chord label for the time frame n. \n",
    "The length of the list depends on the feature rate, i.e., both on the window length and hop size \n",
    "used for the chromagram computation and on the downsampling factor, if feature downsampling is \n",
    "applied.\n",
    "\n",
    "The chord templates to be considered are the major triads and the minor triads, leading to a total of 24 templates.\n",
    "Once the function is defined in the notebook, test the function on the wav file Beatles LetItBe.wav,\n",
    "available in /data/wav/ folder, and print or plot the output.\n",
    "\n",
    "Explain in the report the idea behind the template-based chord recognition algorithm and detail each\n",
    "step implemented in the code, including pre processing and post processing phases.\n",
    "'''\n",
    "template_cmaj = np.array([[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "template_cmin = np.array([[1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "\n",
    "def generate_template_matrix(templates):\n",
    "    \n",
    "    template_matrix = np.zeros((12, 12 * templates.shape[1]))\n",
    "\n",
    "    for shift in range(12):\n",
    "        #np.roll: roll array elements along a given axis.\n",
    "        template_matrix[:, shift::12] = np.roll(templates, shift, axis=0)\n",
    "\n",
    "    return template_matrix\n",
    "\n",
    "\n",
    "# column vectors\n",
    "\n",
    "templates = generate_template_matrix(np.concatenate((template_cmaj, template_cmin), axis=1))\n",
    "\n",
    "\n",
    "chroma_label = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "chord_label_maj = chroma_label\n",
    "chord_label_min = [s + 'm' for s in chroma_label]\n",
    "chord_labels = chord_label_maj + chord_label_min\n",
    "\n",
    "#fig = plt.figure(figsize=(10, 4))\n",
    "#plt.imshow(templates, origin='lower', aspect='auto', cmap='bwr', extent=[0, 24, 0, 12])\n",
    "#plt.clim([-1, 1])\n",
    "#plt.xlabel('Chord')\n",
    "#plt.xticks(np.arange(24) + 0.5, chord_labels)\n",
    "#plt.ylabel('Pitch class')\n",
    "#plt.yticks(np.arange(12) + 0.5, chroma_label)\n",
    "#plt.axvline(x=12, ymin=0, ymax = 12, linewidth=1, color='k')\n",
    "#plt.tight_layout()\n",
    "\n",
    "def normalize_feature_sequence(X, norm='2', threshold=0.0001, v=None):\n",
    "    \"\"\"Normalizes the columns of a feature sequence\n",
    "\n",
    "    Args:\n",
    "        X: Feature sequence\n",
    "        norm: The norm to be applied. '1', '2', 'max'\n",
    "        threshold: An+ threshold below which the vector `v` used instead of normalization\n",
    "        v: Used instead of normalization below `threshold`. If None, uses unit vector for given norm\n",
    "\n",
    "    Returns:\n",
    "        X_norm: Normalized feature sequence\n",
    "    \"\"\"\n",
    "    K, N = X.shape\n",
    "    X_norm = np.zeros((K, N))\n",
    "\n",
    "    if norm == '1':\n",
    "        if v is None:\n",
    "            v = np.ones(K, dtype=np.float64) / K\n",
    "        for n in range(N):\n",
    "            s = np.sum(np.abs(X[:, n]))\n",
    "            if s > threshold:\n",
    "                X_norm[:, n] = X[:, n] / s\n",
    "            else:\n",
    "                X_norm[:, n] = v\n",
    "\n",
    "    if norm == '2':\n",
    "        if v is None:\n",
    "            v = np.ones(K, dtype=np.float64) / np.sqrt(K)\n",
    "        for n in range(N):\n",
    "            s = np.sqrt(np.sum(X[:, n] ** 2))\n",
    "            if s > threshold:\n",
    "                X_norm[:, n] = X[:, n] / s\n",
    "            else:\n",
    "                X_norm[:, n] = v\n",
    "                \n",
    "    if norm == 'max':\n",
    "        if v is None:\n",
    "            v = np.ones(K)\n",
    "        for n in range(N):\n",
    "            s = np.max(np.abs(X[:, n]))\n",
    "            if s > threshold:\n",
    "                X_norm[:, n] = X[:, n] / s\n",
    "            else:\n",
    "                X_norm[:, n] = v\n",
    "\n",
    "    return X_norm\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "def smooth_downsample_feature_sequence(X, Fs, filt_len=41, down_sampling=10, w_type='boxcar'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: Feature sequence\n",
    "        Fs: Frame rate of `X`\n",
    "        filt_len: Length of smoothing filter\n",
    "        down_sampling: Downsampling factor\n",
    "        w_type: Window type of smoothing filter\n",
    "\n",
    "    Returns:\n",
    "        X_smooth: Smoothed and downsampled feature sequence\n",
    "        Fs_feature: Frame rate of `X_smooth`\n",
    "    \"\"\"\n",
    "    # use expand dims to add one dimension to the window, from (L, ) to (1,L) \n",
    "    filt_kernel = np.expand_dims(signal.get_window(w_type, filt_len), axis=0)\n",
    "    X_smooth = signal.convolve(X, filt_kernel, mode='same') / filt_len\n",
    "    X_smooth = X_smooth[:, ::down_sampling]\n",
    "    Fs_feature = Fs / down_sampling\n",
    "    return X_smooth, Fs_feature\n",
    "\n",
    "def analysis_template_match(chromagram, templates, smoothing_window_length=None, smoothing_down_sampling=None,\n",
    "                            Fs=22050,\n",
    "                            norm_chromagram='2', norm_output='2'):\n",
    "    \n",
    "    chroma_normalized = normalize_feature_sequence(chromagram, norm=norm_chromagram)\n",
    "    \n",
    "    if smoothing_window_length and smoothing_down_sampling:\n",
    "        chroma_normalized, Fs_feat = smooth_downsample_feature_sequence(chroma_normalized, \n",
    "                                                                        down_sampling=smoothing_down_sampling,\n",
    "                                                                        filt_len=smoothing_window_length,\n",
    "                                                                        Fs=Fs)\n",
    "        \n",
    "    templates_normalized = normalize_feature_sequence(templates, norm=norm_chromagram)\n",
    "    \n",
    "    chord_similarity = np.matmul(templates_normalized.T, chroma_normalized)\n",
    "    \n",
    "    if norm_output:\n",
    "         chord_similarity = normalize_feature_sequence(chord_similarity, norm=norm_output)\n",
    "    \n",
    "    chord_max = (chord_similarity == chord_similarity.max(axis=0)).astype(int)\n",
    "\n",
    "    return chord_similarity, chord_max\n",
    "\n",
    "# load wav file\n",
    "#fn_wav = os.path.join('data', 'audio', 'Beatles_LetItBe.wav')\n",
    "Fs = 22050\n",
    "x, Fs = librosa.load('data/wav/Beatles_ObLaDiObLaDa.wav', sr=Fs)\n",
    "notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B', 'Cm', 'C#m', 'Dm', 'D#m', 'Em', 'Fm', 'F#m', 'Gm', 'G#m', 'Am', 'A#m', 'Bm'];\n",
    "N = 4096\n",
    "H = 2048\n",
    "\n",
    "X = librosa.stft(x, n_fft=N, hop_length=H, pad_mode='constant', center=True)\n",
    "\n",
    "X = np.abs(X) ** 2\n",
    "# a possible additional step is to use logarithmic compression \n",
    "gamma = 0.1\n",
    "X = np.log(1 + gamma * np.abs(X) ** 2)\n",
    "\n",
    "C = librosa.feature.chroma_stft(S=X, sr=Fs, tuning=0, hop_length=H, n_fft=N, norm=None)\n",
    "\n",
    "\n",
    "chords_sim, chords_max = analysis_template_match(C, templates, \n",
    "                                                 smoothing_window_length=None,\n",
    "                                                 smoothing_down_sampling=None,\n",
    "                                                 norm_chromagram='2', \n",
    "                                                 norm_output='max')\n",
    "\n",
    "# Compute normalized binary templates of analysis\n",
    "templates_normalized = normalize_feature_sequence(templates, norm='2')\n",
    "\n",
    "# by multipling the most probable chords by the templates (normalized), you obtain the sequence of chords chroma\n",
    "# by chroma\n",
    "\n",
    "template_sequence = np.matmul(templates_normalized, chords_max)\n",
    "\n",
    "input_list = [] \n",
    " \n",
    "for col in range(chords_max.shape[1]-1): \n",
    "    if 1 in chords_max[:,col]: \n",
    "        index = np.where(chords_max[:,col] == 1)[0][0] \n",
    "        input_list.append(chords[index]) \n",
    "print(input_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "66ed5112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F', 'A', 'A', 'D', 'A#', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'D#', 'A#', 'A#', 'A#', 'A#', 'D#', 'A#', 'F', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'D#', 'A#', 'A#', 'A#', 'A#', 'D#', 'A#', 'F', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'Gm', 'F', 'F', 'F', 'F', 'A#']\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "'''\n",
    "Write a function to load and preprocess a reference annotation (or ground truth) file, saved in CSV\n",
    "format. The function should take as input the path to a CSV file and produce as output a list of ground\n",
    "truth chord labels, after suitable pre processing. The output must be a list\n",
    "\n",
    "λgt = [λgt0, λgt1, ..., λgtN−1] (2)\n",
    "\n",
    "where each element λgtn is the ground truth chord label for the time window n. \n",
    "The length of the list must be adapted to match the the feature rate.\n",
    "\n",
    "The reference annotations stored in the CSV file are given in the form of labelled segments, each\n",
    "specified as a triplet (start, end, λ) where start and end are expressed in seconds. To load the CSV file\n",
    "check the csv library (distributed with Python) or Pandas library (needs to be installed).\n",
    "\n",
    "In the preprocessing step you should\n",
    "• convert the segment-based annotation into a frame-based label sequence adapted to the feature rate\n",
    "used for the chroma sequence;\n",
    "• convert the labels used in the annotation file to match the chord labels used for the chord recognition\n",
    "algorithm in terms of enharmonic equivalence (i.e., Db = C# );\n",
    "\n",
    "• reduce the chord label set used in the annotation to match the chord labels set used for the chord\n",
    "recognition algorithm (i.e., DMaj6 → D); for this step you can choose any reduction strategy.\n",
    "\n",
    "Once the function is defined in the notebook, test the function on the CSV file Beatles LetItBe.csv,\n",
    "available in /data/csv/ folder, and print or plot the output.\n",
    "Explain in the report each step of the preprocessing phase, focusing in particular on the reduction\n",
    "strategy of the chord label set.\n",
    "'''\n",
    "\n",
    "def chordsFromCsv(path):\n",
    "    with open(path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        list = []\n",
    "        i = 0;\n",
    "        note = ['C','D','E','F','G','A','B']\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                if \"min\" in row[2].split(':'):\n",
    "                    temp = row[2].split(':')[0]+'m'\n",
    "                else:\n",
    "                    temp = row[2].split(':')[0]\n",
    "                \n",
    "                list.append(temp.split('/')[0])\n",
    "\n",
    "        for e in list:\n",
    "            if 'b' in e:\n",
    "                list[i]=note[note.index(list[i][0])-1]+'#'\n",
    "            i += 1\n",
    "            \n",
    "    return list\n",
    "\n",
    "path_song='data/csv/Beatles_ObLaDiObLaDa.csv' \n",
    "\n",
    "chords=chordsFromCsv(path_song);\n",
    "\n",
    "print(chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0d3a4931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "'''\n",
    "Propose a metric for evaluating the template based chord recognition algorithm. \n",
    "A metric\n",
    "\n",
    "m = f(λ_pred,λ_gt) (3)\n",
    "\n",
    "is a scalar number that expresses how good is the algorithm in performing the task of chord recognition.\n",
    "The proposed metric should have higher values when the chord recognition algorithm is able to predict\n",
    "correctly the ground truth chords most of the times, lower values if the chord recognition algorithm often\n",
    "fails at recognising the chords.\n",
    "\n",
    "Write a function that takes as input the list of predicted chord labels, the list of ground truth chord\n",
    "labels and computes the proposed metric value. The two input lists must have same length and the\n",
    "output must be a scalar value.\n",
    "\n",
    "Once the function is defined in the notebook, test the function on the two lists of predicted and ground\n",
    "truth chord labels computed in Question 1 and Question 2.\n",
    "\n",
    "In the report write a formal definition of the proposed metric and thoroughly explain the idea behind\n",
    "the proposal. Can you imagine a musically informed strategy that weights differently mismatch errors of\n",
    "the chord recognition algorithm?\n",
    "'''\n",
    "#notes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];\n",
    "trueChords=chordsFromCsv(path_song);\n",
    "\n",
    "#recognizedChords=chordList(templates, template_sequence);\n",
    "#da ultimare\n",
    "\n",
    "recognizedChords=['F', 'A', 'A', 'D', 'A#', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'D#', 'A#', 'A#', 'A#', 'A#', 'D#', 'A#', 'F', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'D#', 'A#', 'A#', 'A#', 'A#', 'D#', 'A#', 'F', 'A#', 'F', 'A#', 'D#', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'A#', 'A#', 'F', 'Gm', 'A#', 'F', 'Gm', 'F', 'F', 'F', 'F', 'A#']\n",
    "numChords=len(trueChords);\n",
    "num_of_error=0;\n",
    "imp_errors=[];\n",
    "\n",
    "for chord in range(len(trueChords)):\n",
    "    #print (chord,trueChords[chord])\n",
    "    if(trueChords[chord]!=recognizedChords[chord]):\n",
    "        num_of_error += 1;\n",
    "        print(chord,trueChords[chord]);\n",
    "        imp_errors.append(abs(notes.index(trueChords[chord])- notes.index(recognizedChords[chord])));\n",
    "        #calcolare la quantita dell'errore\n",
    "\n",
    "if (num_of_error!=0):\n",
    "    perc_success1=math.trunc((100-((num_of_error/numChords)*100)));\n",
    "    imp_errors_med=sum(imp_errors)/len(imp_errors);\n",
    "    perc_success2=math.trunc((100-((imp_errors_med/11)*100)));\n",
    "    print(perc_success1,perc_success2);\n",
    "    \n",
    "else:\n",
    "    print(100);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313793b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the proposed metric for the remaining 3 songs:\n",
    "• audio Beatles HereComesTheSun.wav, CSV Beatles HereComesTheSun.csv\n",
    "• audio Beatles PennyLane.wav, CSV Beatles PennyLane.csv\n",
    "• audio Beatles ObLaDiObLaDa.wav, CSV Beatles ObLaDiObLaDa.csv\n",
    "all contained in folders /data/wav/ and /data/csv/. Print or plot the metric values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Analyse how algorithm parameters affect the performance of the templated based chord recognition\n",
    "algorithm.\n",
    "Given one algorithm parameter (i.e., smoothing filter length L), choose a range of 3 possible values\n",
    "for it (i.e.,L = [0, 10, 20] ). \n",
    "For each value of the parameter, compute the predicted labels and the correspondent metric value for each song.\n",
    "This produces a list of 3 metric values for each song.\n",
    "\n",
    "Plot the results for all songs in a figure where the x-axis corresponds to the parameter values, the\n",
    "y-axis corresponds to metric values. An example of the plot is in Figure 1.\n",
    "\n",
    "Repeat this for at least 3 different algorithm parameters. \n",
    "Note that in every experiment you need to change only one parameter value at a time, while the others must be fixed.\n",
    "\n",
    "Show the 3 plots both in the Jupyter Notebook and in the report. \n",
    "What considerations can you do from the 3 plots that you have? \n",
    "Are some algorithm parameters affecting the results more than others?\n",
    "Comment the results in the report.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
